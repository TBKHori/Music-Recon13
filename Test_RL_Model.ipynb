{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZlwmhJ2XEKIkoNVpgRabi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TBKHori/Music-Recon13/blob/main/Test_RL_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable_baselines3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1SuH8I_9upK",
        "outputId": "93e5d967-595b-4f55-a324-ac170b2b652e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-2.0.0-py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.4/178.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium==0.28.1 (from stable_baselines3)\n",
            "  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.0.1+cu118)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n",
            "Collecting jax-jumpy>=1.0.0 (from gymnasium==0.28.1->stable_baselines3)\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->stable_baselines3) (4.7.1)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium==0.28.1->stable_baselines3)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable_baselines3) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable_baselines3) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable_baselines3) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable_baselines3) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable_baselines3) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11->stable_baselines3) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11->stable_baselines3) (16.0.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->stable_baselines3) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11->stable_baselines3) (1.3.0)\n",
            "Installing collected packages: farama-notifications, jax-jumpy, gymnasium, stable_baselines3\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.28.1 jax-jumpy-1.0.0 stable_baselines3-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f_9z2_j78p-6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pytest"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import A2C, DDPG, DQN, PPO, SAC, TD3\n",
        "from stable_baselines3.common.envs import IdentityEnv, IdentityEnvBox, IdentityEnvMultiBinary, IdentityEnvMultiDiscrete\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.noise import NormalActionNoise\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv"
      ],
      "metadata": {
        "id": "qf2XaI0u993m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DIM = 4"
      ],
      "metadata": {
        "id": "QZsv2c_KADji"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@pytest.mark.parametrize(\"model_class\", [A2C, PPO, SAC, DDPG, TD3])\n",
        "def test_continuous(model_class):\n",
        "    env = IdentityEnvBox(eps=0.5)\n",
        "\n",
        "    n_steps = {A2C: 2000, PPO: 2000, SAC: 400, TD3: 400, DDPG: 400}[model_class]\n",
        "\n",
        "    kwargs = dict(policy_kwargs=dict(net_arch=[64, 64]), seed=0, gamma=0.95)\n",
        "\n",
        "    if model_class in [TD3]:\n",
        "        n_actions = 1\n",
        "        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "        kwargs[\"action_noise\"] = action_noise\n",
        "    elif model_class in [A2C]:\n",
        "        kwargs[\"policy_kwargs\"][\"log_std_init\"] = -0.5\n",
        "    elif model_class == PPO:\n",
        "        kwargs = dict(n_steps=512, n_epochs=5)\n"
      ],
      "metadata": {
        "id": "98u-T81f-E_y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_discrete(model_class, env):\n",
        "    env_ = DummyVecEnv([lambda: env])\n",
        "    kwargs = {}\n",
        "    n_steps = 2500\n",
        "    if model_class == DQN:\n",
        "        kwargs = dict(learning_starts=0)\n",
        "        # DQN only support discrete actions\n",
        "        if isinstance(env, (IdentityEnvMultiDiscrete, IdentityEnvMultiBinary)):\n",
        "            return"
      ],
      "metadata": {
        "id": "76Oit4TG-FZU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_class is the class of the RL model to be used, e.g., PPO, A2C, etc.\n",
        "# \"MlpPolicy\" specifies the policy architecture, which is Multi-Layer Perceptron (MLP) in this case.\n",
        "# env_ is the environment in which the RL model will be trained.\n",
        "# gamma is the discount factor used in the RL algorithm.\n",
        "# seed is the random seed for reproducibility.\n",
        "# **kwargs represents any additional hyperparameters that can be passed to the model_class.\n",
        "model = model_class(\"MlpPolicy\", env_, gamma=0.4, seed=3, **kwargs).learn(n_steps)\n",
        "\n",
        "# After training, evaluate the trained RL model on the environment env_.\n",
        "# n_eval_episodes specifies the number of episodes for evaluation.\n",
        "# reward_threshold is the minimum reward required for success.\n",
        "# warn=False suppresses warning messages during evaluation.\n",
        "evaluate_policy(model, env_, n_eval_episodes=20, reward_threshold=90, warn=False)\n",
        "\n",
        "# Reset the environment and get the initial observation (state).\n",
        "obs, _ = env.reset()\n",
        "\n",
        "# Assert the shape of the model's prediction is the same as the shape of the observation.\n",
        "# This is a basic check to ensure the model can predict actions based on observations.\n",
        "assert np.shape(model.predict(obs)[0]) == np.shape(obs)\n"
      ],
      "metadata": {
        "id": "k2RD4Xps-Ujq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@pytest.mark.parametrize(\"model_class\", [A2C, PPO, SAC, DDPG, TD3])\n",
        "def test_continuous(model_class):\n",
        "    env = IdentityEnvBox(eps=0.5)\n",
        "\n",
        "    n_steps = {A2C: 2000, PPO: 2000, SAC: 400, TD3: 400, DDPG: 400}[model_class]\n",
        "\n",
        "    kwargs = dict(policy_kwargs=dict(net_arch=[64, 64]), seed=0, gamma=0.95)\n",
        "\n",
        "    if model_class in [TD3]:\n",
        "        n_actions = 1\n",
        "        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "        kwargs[\"action_noise\"] = action_noise\n",
        "    elif model_class in [A2C]:\n",
        "        kwargs[\"policy_kwargs\"][\"log_std_init\"] = -0.5\n",
        "    elif model_class == PPO:\n",
        "        kwargs = dict(n_steps=512, n_epochs=5)"
      ],
      "metadata": {
        "id": "D6DBhnm1-YBb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_class is the class of the RL model to be used, e.g., PPO, A2C, etc.\n",
        "# \"MlpPolicy\" specifies the policy architecture, which is Multi-Layer Perceptron (MLP) in this case.\n",
        "# env is the environment in which the RL model will be trained and evaluated.\n",
        "# learning_rate is the learning rate used by the optimizer during training.\n",
        "# **kwargs represents any additional hyperparameters that can be passed to the model_class.\n",
        "model = model_class(\"MlpPolicy\", env, learning_rate=1e-3, **kwargs).learn(n_steps)\n",
        "\n",
        "# After training, evaluate the trained RL model on the same environment.\n",
        "# n_eval_episodes specifies the number of episodes for evaluation.\n",
        "# reward_threshold is the minimum reward required for success.\n",
        "# warn=False suppresses warning messages during evaluation.\n",
        "evaluate_policy(model, env, n_eval_episodes=20, reward_threshold=90, warn=False)\n"
      ],
      "metadata": {
        "id": "sCw7_cmf_r24"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}